# -*- coding: utf-8 -*-
"""Movie Review Dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JmNpPYuyI9w3S5UGHhGhIPDNX_fGIXvY
"""

# Commented out IPython magic to ensure Python compatibility.
# %autosave 1

import pandas as pd
import numpy as np
import re

import os
import tarfile
import urllib

import nltk
from nltk import word_tokenize
from nltk.corpus import stopwords, wordnet, words
from nltk.stem import WordNetLemmatizer 

from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, recall_score

from IPython.display import display

nltk.download('punkt')
nltk.download('words')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz -O data.tar.gz

# Unzipping the data files.
tar = tarfile.open('data.tar.gz', 'r:gz')
tar.extractall()
tar.close()

"""# Data Cleaninng 
## **Goal:** Get the data in clean, standard format for further analysis.
## **HINT:** Different Types of analysis require different data formats.


1.   Corpus (I will use Pandas to build the data in this format)
2.   Document-Term Matrix

### Actually for Document-Term Matrix Format we need to do some nice things:

*   Cleaning data { numbers, punks, lowercase }
*   Tokenize text ( Spitting the data into tokens then removing the stopwords from tokens )






"""

def collect_data(data_path: str):
  reviews = []
  for review_file in os.listdir(data_path):
    review_file = os.path.join(data_path, review_file)
    with open(review_file, 'r') as review:
      reviews.append(review.read().rstrip())
  return reviews

def clean(data):
  cleaned_data = []
  for item in data:
    item = item.lower()
    item = re.sub(r'[^\w\s]|[\d+]', '', item)
    cleaned_data.append(item)
  return cleaned_data

pos_training_data_path = '/content/aclImdb/train/pos'
neg_training_data_path = '/content/aclImdb/train/neg'
pos_testing_data_path = '/content/aclImdb/test/pos'
neg_testing_data_path = '/content/aclImdb/test/neg'

train_pos_reviews = collect_data(pos_training_data_path)
train_neg_reviews = collect_data(neg_training_data_path)
test_pos_reviews = collect_data(pos_testing_data_path)
test_neg_reviews = collect_data(neg_testing_data_path)

cleaned_train_pos_reviews = clean(train_pos_reviews)
cleaned_train_neg_reviews = clean(train_neg_reviews)
cleaned_test_pos_reviews = clean(test_pos_reviews)
cleaned_test_neg_reviews = clean(test_neg_reviews)
  
print('Number of positive reviews: {}, and the number of negative reviews is: {}'.format(len(train_pos_reviews), len(train_neg_reviews)))
print('Number of positive reviews: {}, and the number of negative reviews is: {}'.format(len(test_pos_reviews), len(test_neg_reviews)))

train_pos_reviews[0]

cleaned_train_pos_reviews[0]

# Preparing the stopwards in English
english_stopwords = stopwords.words('english')
# Lemmatization
lemmatizer = WordNetLemmatizer()
# Error and spelling. corrections
words = set(nltk.corpus.words.words())

def get_wordnet_pos(word):
    """Map POS tag to first character lemmatize() accepts"""
    tag = nltk.pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ,
                "N": wordnet.NOUN,
                "V": wordnet.VERB,
                "R": wordnet.ADV}

    return tag_dict.get(tag, wordnet.NOUN)

def advanced_clean(data, potential_features: set):
  for idx, sample in enumerate(data):
    __tokens__ = []
    tokens = nltk.word_tokenize(sample)
    for token in tokens:
      if token not in english_stopwords:
        token_pos = get_wordnet_pos(token)
        lemma = lemmatizer.lemmatize(token, token_pos)
        if  lemma in words and lemma not in english_stopwords and lemma in words:
          potential_features.add(lemma)
          __tokens__.append(lemma)
    data[idx] = ''.join(__token__ + ' ' for __token__ in __tokens__)

# converting the representation of text to the Bag of Words (BoW) model.
# BoW is a simplified format of representing the text that ingnrs order.
# Really powerful as a first round of analysis.
potential_features = set()

advanced_clean(cleaned_train_pos_reviews, potential_features)
advanced_clean(cleaned_train_neg_reviews, potential_features)
advanced_clean(cleaned_test_pos_reviews, potential_features)
advanced_clean(cleaned_test_neg_reviews, potential_features)

print("number of features(unique english non-stopwords words):", len(potential_features))

"""# Building Corups"""

# preparring the labels
labels = np.array([1 for item in range(len(train_pos_reviews))] + [0 for item in range(len(train_neg_reviews))])

data = {
    'reviews': train_pos_reviews + train_neg_reviews,
    'label': labels
}
corpus_df = pd.DataFrame(data=data).sample(frac = 1).reset_index(drop=True)

corpus_df.head(10)

"""# Data splitting"""

full_data = cleaned_train_pos_reviews + cleaned_train_neg_reviews + cleaned_test_pos_reviews + cleaned_test_neg_reviews
labels = np.array([1 for item in range(len(cleaned_train_pos_reviews))] + [0 for item in range(len(cleaned_train_neg_reviews))] 
                  + [1 for item in range(len(cleaned_test_pos_reviews))] + [0 for item in range(len(cleaned_test_neg_reviews))])

len(full_data), len(labels)

full_train, test, full_train_labels, test_labels = train_test_split(full_data, labels, test_size=0.2, shuffle=True, random_state=11);
train, validate, train_labels, validate_labels = train_test_split(full_train, full_train_labels, test_size=0.25, shuffle=True, random_state=11);

len(train), len(train_labels), len(validate), len(validate_labels), len(test), len(test_labels)

"""# DTM data frame"""

cv = CountVectorizer(max_df=0.95, min_df=0.05)
X_train_data = cv.fit_transform(train)
X_validate_data = cv.transform(validate)
X_test_data = cv.transform(test)

X_train_data.toarray().shape

len(cv.get_feature_names())

train_df = pd.DataFrame(data=X_train_data.toarray(), columns=cv.get_feature_names())
validate_df = pd.DataFrame(X_validate_data.toarray(), columns = cv.get_feature_names())
test_df = pd.DataFrame(X_test_data.toarray(), columns = cv.get_feature_names())

display(train_df.head()), display(validate_df.head()), display(test_df.head())

from sklearn import preprocessing
X_train = pd.DataFrame(preprocessing.scale(X_train_data.toarray()), columns = cv.get_feature_names())
X_validate = pd.DataFrame(preprocessing.scale(X_validate_data.toarray()), columns = cv.get_feature_names())
X_test = pd.DataFrame(preprocessing.scale(X_test_data.toarray()), columns = cv.get_feature_names())

pca = PCA(n_components=0.8, svd_solver='full')
X_train_minimized = pca.fit_transform(X_train)
X_validate_minimized = pca.transform(X_validate)
X_test_minimized = pca.transform(X_test)

train_df2 = pd.DataFrame(data=X_train_minimized)
validate_df2 = pd.DataFrame(X_validate_minimized)
test_df2 = pd.DataFrame(X_test_minimized)

display(train_df2.head()), display(validate_df2.head()), display(test_df2.head())

"""# Training

## KNN Classifier

### A quick prototype
"""

knn_classifier = KNeighborsClassifier(n_neighbors=3)
knn_classifier.fit(train_df, train_labels)

y_pred_proba = knn_classifier.predict_proba(validate_df)[:, 1]
accuracy_score(validate_labels, (y_pred_proba >= 0.5))

"""### Hyperparameter tuning"""

optmization = {
    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],
    'n_neighbors': [1, 2, 3]
}

_knn = KNeighborsClassifier()
gscv = GridSearchCV(_knn, optmization, scoring='accuracy',cv=4)
lr_gs = gscv.fit(train_df, train_labels)

print('Accurecy score: %.5f' % lr_gs.best_score_)
print('Best n_neighbors: %s' % lr_gs.best_params_)

"""### Training with the best combination"""

_knn_best = KNeighborsClassifier(**lr_gs.best_params_)

_knn_best.fit(train_df, train_labels)

y_pred_proba = _knn_best.predict_proba(validate_df)[:, 1]
accuracy_score(validate_labels, (y_pred_proba >= 0.5))

"""## XGBoost

### A quick prototype
"""

import xgboost as xgb

dtrain = xgb.DMatrix(data=train_df, label=train_labels)
xgb_params = {
    'eta': 0.01, 
    'max_depth': 6,
    'min_child_weight': 1,
    
    'objective': 'binary:logistic',
    'nthread': 8,
    
    'seed': 1,
    'verbosity': 1,
    'eval_metric': 'auc'
}

model = xgb.train(xgb_params, dtrain, num_boost_round=100)

dtest = xgb.DMatrix(data=validate_df, label=validate_labels)
y_pred_proba = model.predict(dtest)
accuracy_score(validate_labels, (y_pred_proba >= 0.5))

"""### Hyperparameter tuning"""

xgb_params = {"eta":[0.01, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30],
               "max_depth":[ 3, 4, 5, 6, 8, 10, 12, 15],
               "min_child_weight":[ 1, 3, 5, 7 ],
               "gamma":[ 0.0, 0.1, 0.2 , 0.3, 0.4 ]
              }

xgbC = xgb.XGBClassifier(objective="binary:logistic", use_label_encoder=False, eval_metric='auc')
xgbc_random = RandomizedSearchCV(estimator = xgbC, 
                                    param_distributions = xgb_params, 
                                    n_iter = 100,
                                    scoring = 'roc_auc',
                                    cv = 4, 
                                    verbose=2, 
                                    random_state=11);

# Fit the random search model
xgbc_random.fit(train_df, train_labels);

"""### Fitting with tuned hyperparameters"""

dtrain = xgb.DMatrix(data=train_df, label=train_labels)
xgb_params = {
    'eta': 0.3, 
    'max_depth': 6,
    'min_child_weight': 1,
    
    'objective': 'binary:logistic',
    'nthread': 8,
    
    'seed': 86431,
    'verbosity': 1,
    'eval_metric': 'auc'
}

xgbC = xgb.train(xgb_params, dtrain, num_boost_round=100)

dtest = xgb.DMatrix(data=X_test, label=y_test)
y_pred_proba = xgbC.predict(dtest)
roc_auc_score(y_test, y_pred_proba)

dtest = xgb.DMatrix(data=validate_df, label=validate_labels)
y_pred_proba = xgbC.predict(dtest)
accuracy_score(validate_labels, (y_pred_proba >= 0.5))

"""# Already given BoW"""

# Loading feature names from the "imdb.vocab" file
with open('/content/aclImdb/imdb.vocab', 'r') as feature_names:
  given_features = [given_feature.rstrip() for given_feature in feature_names if not given_feature.rstrip() in english_stopwords]

len(given_features)

# Reading labeledBow.feat file which contains the data in libsvm format (sparse matrix)
# shape of data <Label> <Feature>:<Number of Apperance>
POSITIVE_REVIEW = 7
review_tokens = []
labels = []
with open('/content/aclImdb/train/labeledBow.feat', 'r') as data:
  for record in data:
    record = record.split()
    labels.append(int(record[0]) >= POSITIVE_REVIEW)
    review_tokens.append(record[1:])

# Extracting the words of each review coupled with its number of apperance.
# Takeing care of the stop words which isnot needed.
tuples = []
for record in review_tokens:
  sentance_tuple = {}
  for item in record:
    feature, value = item.split(':')
    if not features[int(feature)] in english_stopwords:
      sentance_tuple[features[int(feature)]] = int(value)
  tuples.append(sentance_tuple)
print("Number of trainig records: ", len(tuples))

"""# Model Saving and Loading"""

import pickle

model_name = 'xgb_model'
model_file = f'model_{model_name}.bin'

with open(model_file, 'wb') as m_out:
  pickle.dump(xgbC, m_out)

