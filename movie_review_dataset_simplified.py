# -*- coding: utf-8 -*-
"""Movie Review Dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JmNpPYuyI9w3S5UGHhGhIPDNX_fGIXvY
"""

# Commented out IPython magic to ensure Python compatibility.
# %autosave 1

import pandas as pd
import numpy as np
import re

import os
import tarfile
import urllib

import nltk
from nltk import word_tokenize
from nltk.corpus import stopwords, wordnet, words
from nltk.stem import WordNetLemmatizer 

from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, recall_score

from IPython.display import display

nltk.download('punkt')
nltk.download('words')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

def collect_data(data_path: str):
  reviews = []
  for review_file in os.listdir(data_path):
    review_file = os.path.join(data_path, review_file)
    with open(review_file, 'r') as review:
      reviews.append(review.read().rstrip())
  return reviews

def clean(data):
  cleaned_data = []
  for item in data:
    item = item.lower()
    item = re.sub(r'[^\w\s]|[\d+]', '', item)
    cleaned_data.append(item)
  return cleaned_data

pos_training_data_path = '/content/aclImdb/train/pos'
neg_training_data_path = '/content/aclImdb/train/neg'
pos_testing_data_path = '/content/aclImdb/test/pos'
neg_testing_data_path = '/content/aclImdb/test/neg'

train_pos_reviews = collect_data(pos_training_data_path)
train_neg_reviews = collect_data(neg_training_data_path)
test_pos_reviews = collect_data(pos_testing_data_path)
test_neg_reviews = collect_data(neg_testing_data_path)

cleaned_train_pos_reviews = clean(train_pos_reviews)
cleaned_train_neg_reviews = clean(train_neg_reviews)
cleaned_test_pos_reviews = clean(test_pos_reviews)
cleaned_test_neg_reviews = clean(test_neg_reviews)

# Preparing the stopwards in English
english_stopwords = stopwords.words('english')
# Lemmatization
lemmatizer = WordNetLemmatizer()
# Error and spelling. corrections
words = set(nltk.corpus.words.words())

def get_wordnet_pos(word):
    """Map POS tag to first character lemmatize() accepts"""
    tag = nltk.pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ,
                "N": wordnet.NOUN,
                "V": wordnet.VERB,
                "R": wordnet.ADV}

    return tag_dict.get(tag, wordnet.NOUN)

def advanced_clean(data, potential_features: set):
  for idx, sample in enumerate(data):
    __tokens__ = []
    tokens = nltk.word_tokenize(sample)
    for token in tokens:
      if token not in english_stopwords:
        token_pos = get_wordnet_pos(token)
        lemma = lemmatizer.lemmatize(token, token_pos)
        if  lemma in words and lemma not in english_stopwords and lemma in words:
          potential_features.add(lemma)
          __tokens__.append(lemma)
    data[idx] = ''.join(__token__ + ' ' for __token__ in __tokens__)

# converting the representation of text to the Bag of Words (BoW) model.
# BoW is a simplified format of representing the text that ingnrs order.
# Really powerful as a first round of analysis.
potential_features = set()

advanced_clean(cleaned_train_pos_reviews, potential_features)
advanced_clean(cleaned_train_neg_reviews, potential_features)
advanced_clean(cleaned_test_pos_reviews, potential_features)
advanced_clean(cleaned_test_neg_reviews, potential_features)

"""Data splitting"""

full_data = cleaned_train_pos_reviews + cleaned_train_neg_reviews + cleaned_test_pos_reviews + cleaned_test_neg_reviews
labels = np.array([1 for item in range(len(cleaned_train_pos_reviews))] + [0 for item in range(len(cleaned_train_neg_reviews))] 
                  + [1 for item in range(len(cleaned_test_pos_reviews))] + [0 for item in range(len(cleaned_test_neg_reviews))])

len(full_data), len(labels)

full_train, test, full_train_labels, test_labels = train_test_split(full_data, labels, test_size=0.2, shuffle=True, random_state=11);
train, validate, train_labels, validate_labels = train_test_split(full_train, full_train_labels, test_size=0.25, shuffle=True, random_state=11);

len(train), len(train_labels), len(validate), len(validate_labels), len(test), len(test_labels)

"""DTM data frame"""

cv = CountVectorizer(max_df=0.95, min_df=0.05)
X_train_data = cv.fit_transform(train)
X_validate_data = cv.transform(validate)
X_test_data = cv.transform(test)

X_train_data.toarray().shape

len(cv.get_feature_names())

train_df = pd.DataFrame(data=X_train_data.toarray(), columns=cv.get_feature_names())
validate_df = pd.DataFrame(X_validate_data.toarray(), columns = cv.get_feature_names())
test_df = pd.DataFrame(X_test_data.toarray(), columns = cv.get_feature_names())

display(train_df.head()), display(validate_df.head()), display(test_df.head())

import xgboost as xgb

dtrain = xgb.DMatrix(data=train_df, label=train_labels)
xgb_params = {
    'eta': 0.3, 
    'max_depth': 6,
    'min_child_weight': 1,
    
    'objective': 'binary:logistic',
    'nthread': 8,
    
    'seed': 86431,
    'verbosity': 1,
    'eval_metric': 'auc'
}

xgbC = xgb.train(xgb_params, dtrain, num_boost_round=100)

dtest = xgb.DMatrix(data=X_test, label=y_test)
y_pred_proba = xgbC.predict(dtest)
roc_auc_score(y_test, y_pred_proba)

dtest = xgb.DMatrix(data=validate_df, label=validate_labels)
y_pred_proba = xgbC.predict(dtest)
accuracy_score(validate_labels, (y_pred_proba >= 0.5))

"""Model Saving and Loading"""

import pickle

model_name = 'xgb_model'
model_file = f'model_{model_name}.bin'

with open(model_file, 'wb') as m_out:
  pickle.dump((cv, xgbC), m_out)

